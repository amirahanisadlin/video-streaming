{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1613538458904,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "A1q91G2o271z"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream, Cursor\n",
    "from tweepy.streaming import StreamListener\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4089,
     "status": "ok",
     "timestamp": 1613538987376,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "H5Z2Te7kZZOR"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Desktop/AssignmentSMC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = json.loads(open('secrets.json').read())\n",
    "api_key = secrets['CONSUMER_KEY']\n",
    "api_secret_key = secrets['CONSUMER_SECRET']\n",
    "access_token = secrets['ACCESS_TOKEN']\n",
    "access_token_secret = secrets['ACCESS_TOKEN_SECRET']\n",
    "\n",
    "auth = OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1303,
     "status": "ok",
     "timestamp": 1613538515612,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "NAEroPEk8Rmg"
   },
   "outputs": [],
   "source": [
    "screen_name = \"hulu\" #your brand name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRIENDS = 1000000\n",
    "followers = []\n",
    "friends = []\n",
    "followers_info = []\n",
    "friends_info = []\n",
    "\n",
    "def paginate(items, n):\n",
    "    \"\"\"Generate n-sized chunks from items\"\"\"\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    screen_name = \"hulu\" #enter screen name on who you would like to check on it.\n",
    "    print(\"collecting data for \" + screen_name)\n",
    "    dirname = \"Assignment/users/{}\".format(screen_name)\n",
    "    max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
    "    try:\n",
    "        os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
    "    except OSError:\n",
    "        print(\"Directory {} already exists\".format(dirname))\n",
    "    except Exception as e:\n",
    "        print(\"Error while creating directory {}\".format(dirname))\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    \"\"\"\n",
    "    # get user's profile\n",
    "    fname = \"Assignment/users/{}/user_profile.json\".format(screen_name)\n",
    "    with open(fname, 'w') as f:\n",
    "        profile = api.get_user(screen_name=screen_name)        \n",
    "        f.write(json.dumps(profile._json, indent=4))\n",
    "    \"\"\"\n",
    "    \n",
    "    # get followers for a given user\n",
    "    print(\"Followers list\")\n",
    "    fname = \"Assignment/users/{}/followers.json\".format(screen_name)\n",
    "    with open(fname, 'w') as f:\n",
    "        for follower in Cursor(api.followers_ids, screen_name=screen_name).pages(max_pages):\n",
    "            \n",
    "            for chunk in paginate(follower, 100):\n",
    "                users = api.lookup_users(user_ids=chunk)\n",
    "                for user in users:\n",
    "                    followers_info = {'Name': user.screen_name, 'Followers': user.followers_count,'Friends': user.friends_count,'Created_at': user.created_at,'Location': user.location}\n",
    "                    followers.append(followers_info)\n",
    "                    f.write(json.dumps(user._json)+\"\\n\")                 \n",
    "            if len(follower) == 5000:\n",
    "                print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "    with open('hulu_followers.csv', 'a', newline='', encoding='utf-8') as file2:\n",
    "        writer = csv.DictWriter(file2, fieldnames=['Name', 'Followers', 'Friends','Created_at','Location'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for data in followers:\n",
    "            writer.writerow(data)\n",
    "\n",
    "    print(\"task completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRIENDS = 1000000\n",
    "followers = []\n",
    "friends = []\n",
    "followers_info = []\n",
    "friends_info = []\n",
    "\n",
    "def paginate(items, n):\n",
    "    \"\"\"Generate n-sized chunks from items\"\"\"\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    screen_name = \"hulu\" #enter screen name on who you would like to check on it.\n",
    "    print(\"Collecting data for \" + screen_name)\n",
    "    dirname = \"Assignment/users/{}\".format(screen_name)\n",
    "    max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
    "    try:\n",
    "        os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
    "    except OSError:\n",
    "        print(\"Directory {} already exists\".format(dirname))\n",
    "    except Exception as e:\n",
    "        print(\"Error while creating directory {}\".format(dirname))\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    \"\"\"\n",
    "    # get user's profile\n",
    "    fname = \"Assignment/users/{}/user_profile.json\".format(screen_name)\n",
    "    with open(fname, 'w') as f:\n",
    "        profile = api.get_user(screen_name=screen_name)        \n",
    "        f.write(json.dumps(profile._json, indent=4))\n",
    "    \"\"\"\n",
    "\n",
    "    # get friends for a given user\n",
    "    print(\"\\n\")\n",
    "    print(\"Friends list\")\n",
    "    fname = \"Assignment/users/{}/friends.json\".format(screen_name)\n",
    "    with open(fname, 'w') as f:\n",
    "        for friend in Cursor(api.friends_ids, screen_name=screen_name).pages(max_pages):\n",
    "            for chunk in paginate(friend, 100):\n",
    "                users = api.lookup_users(user_ids=chunk)\n",
    "                for user in users:\n",
    "                    friends_info = {'Name': user.screen_name, 'Followers': user.followers_count,'Friends': user.friends_count, 'Created_at': user.created_at,'Location': user.location}\n",
    "                    friends.append(friends_info)\n",
    "                    f.write(json.dumps(user._json)+\"\\n\")\n",
    "            if len(friend) == 5000:\n",
    "                print(\"More results available. Sleeping for 60 seconds to#  avoid rate limit\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "    with open('hulu_friends.csv', 'a', newline='', encoding='utf-8') as file2:\n",
    "        writer = csv.DictWriter(file2, fieldnames=['Name', 'Followers', 'Friends','Created_at','Location'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for data in friends:\n",
    "            writer.writerow(data)\n",
    "              \n",
    "    print(\"Task completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFoMzDfh3PrK"
   },
   "source": [
    "# Followers Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1613538650530,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "ewqPakKl5GXR"
   },
   "outputs": [],
   "source": [
    "user = api.get_user(screen_name)\n",
    "fll = user.followers_count\n",
    "fstr = str(fll)\n",
    "date = datetime(2021, 2, 8).strftime('%Y-%m-%d')\n",
    "header = ['Date Collected', 'Count']\n",
    "rows = [date, fll]\n",
    "\n",
    "with open('hulu_followers.csv', 'a+') as csv_file:\n",
    "    write = csv.writer(csv_file)\n",
    "    #write.writerow(header)\n",
    "    write.writerow(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "livA6KUe-6nd"
   },
   "source": [
    "# Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime(2021, 2, 26)\n",
    "tomorrow = date + timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-02-27'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strftime(tomorrow, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1613541511462,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "IW7VEANdZr2_"
   },
   "outputs": [],
   "source": [
    "# Using Cursor to get more than 100 tweets at one time\n",
    "query = '@hulu -filter:retweets'\n",
    "startDate = datetime.strftime(date, '%Y-%m-%d')\n",
    "endDate = datetime.strftime(tomorrow, '%Y-%m-%d')\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query, since=startDate, until=endDate).items()]\n",
    "\n",
    "# JSON\n",
    "with open('hulu_mentions.json', 'w') as f:\n",
    "  for tweet in searched_tweets:\n",
    "    f.write(json.dumps(tweet._json) + '\\n')\n",
    "\n",
    "# # After 14 days, convert json to CSV\n",
    "# df = pd.read_json (r'iqiyi_mentions.json', lines=True)\n",
    "# export_csv = df.to_csv (r'converted.csv', index = None, header=True)\n",
    "\n",
    "df = pd.read_json (r'hulu_mentions.json', lines=True)\n",
    "export_csv = df.to_csv (r'hulu_mentions.csv', index=None, mode='a', header=False) # For the first time, header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json (r'hulu_mentions_9feb.json', lines=True)\n",
    "export_csv = df.to_csv (r'hulu_mentions2.csv', index=None, mode='a', header=True) # For the first time, header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1613541636625,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "0aW-cvGBkvVK"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('converted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1613541861269,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "s7Xp1bqIlhzf"
   },
   "outputs": [],
   "source": [
    "# Convert Object to DateTime (After 14 Days)\n",
    "# df['created_at'] = pd.to_datetime(df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1613541869180,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "sCRYY5aQk4O_"
   },
   "outputs": [],
   "source": [
    "# Split Date & Time (After 14 days)\n",
    "# df['date'] = [d.date() for d in df['created_at']]\n",
    "# df['time'] = [d.time() for d in df['created_at']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AvtVJ27_EZT"
   },
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1544,
     "status": "ok",
     "timestamp": 1613542957642,
     "user": {
      "displayName": "WAN ZULMUHAMMAD HARITH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbRoHVWUl_K_5E-h1CCLVB_pvHDLox3nU1TBfg=s64",
      "userId": "12583059528850556736"
     },
     "user_tz": -480
    },
    "id": "kTbKyYM26Ici",
    "outputId": "af44b5ff-ce7a-44f8-a782-b3d1a79c3b0c"
   },
   "outputs": [],
   "source": [
    "# tweetss = api.user_timeline(screen_name, since=startDate, until=endDate)\n",
    "# i = 1\n",
    "\n",
    "# for twit in tweetss:\n",
    "#   if twit.lang == \"en\":\n",
    "#     print(i, twit.text, \"|\", twit.created_at.time())\n",
    "#     i += 1\n",
    "\n",
    "# with open('hulu_tweets.json', 'w') as f:\n",
    "#   for tweet in tweetss:\n",
    "#     f.write(json.dumps(tweet._json) + '\\n')\n",
    "\n",
    "# df = pd.read_json (r'hulu_tweets.json', lines=True)\n",
    "# export_csv = df.to_csv (r'hulu_tweets.csv', index = None, mode='a', header=False) # For the first time, header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetss = [status for status in tweepy.Cursor(api.user_timeline, screen_name=screen_name, since_id=1359109793139617793, until_id=).items()]\n",
    "\n",
    "with open('hulu_tweets2.json', 'w') as f:\n",
    "  for tweet in tweetss:\n",
    "    f.write(json.dumps(tweet._json) + '\\n')\n",
    "\n",
    "df = pd.read_json (r'hulu_tweets2.json', lines=True)\n",
    "export_csv = df.to_csv (r'hulu_tweets2.csv', index = None, mode='a', header=True) # For the first time, header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aKZcQ7RVYWA8"
   ],
   "name": "Assignment_V2.0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
